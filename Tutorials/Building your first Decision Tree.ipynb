{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your first Decision Tree\n",
    "\n",
    "In this tutorial, we will walk you through all the steps involved in building a decision tree: formulating your data, one-hot encoding data, training a decision tree model and validating the accuracy of the decision tree built, on your test data set. We will be using sklearn's DecisionTreeClassifier API to build the Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulating the data\n",
    "\n",
    "In this section, we will discuss the types of data formulation needed to use the API. Consider the mushroom edibility dataset which contains various features for each mushroom entry such as odour, habitat, etc. There are a total of 22 such features There are a total of 8124 data points in this dataset.\n",
    "\n",
    "The following lines of code are loading the relevant data and organising the data into the features and labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Loading the data from CSV file\n",
    "data = pd.read_csv(\"mushrooms.csv\")\n",
    "\n",
    "# The first column of the csv file contains the labels\n",
    "y = data.iloc[:, 0] # All rows, 0th column\n",
    "\n",
    "# The rest of the columns of the csv file contains the features\n",
    "X = data.iloc[:,1:] # All rows, column 1 to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features of Row 1:  ['x' 's' 'y' 't' 'a' 'f' 'c' 'b' 'k' 'e' 'c' 's' 's' 'w' 'w' 'p' 'w' 'o'\n",
      " 'p' 'n' 'n' 'g']\n",
      "Class label of Row 1:  e\n",
      "Dimensions of X:  (8124, 22)\n"
     ]
    }
   ],
   "source": [
    "# Let us look at what a row from the data contains\n",
    "\n",
    "print \"Features of Row 1: \", X.iloc[1].values\n",
    "print \"Class label of Row 1: \",y.iloc[1]\n",
    "\n",
    "# Let us see the shape of X\n",
    "print \"Dimensions of X: \", X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been loaded and as you can see, each feature is represented by a character. All the features are categorical. Categorical data is one where the feature can take one value out of a discrete set of values. For example, in this case Cap-surface feature can be one of the following: 'fibrous', 'grooves', 'scaly', 'smooth' and are represented as characters in the dataset. In contrast, numerical data such as length can take any set of numerical values in a given range. Also, as expected, X has 22 columns which are the number of features that are there in the dataset.\n",
    "\n",
    "Moving forward, there are some requirements that your data needs to satisfy before being ready to be used for creating the decision tree. Firstly, sklearn's Decision tree classifier requires numeric data to be passed to it. Therefore, it is necessary to model the categorical data, represented by characters, into numeric class labels. This is done by mapping the features into corresponding numerical values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features of Row 1 after numerical encoding [0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Enter code to map the categorical features into numerical values\n",
    "\n",
    "for col in X.columns:\n",
    "    X.loc[:, col] = X.loc[:, col].map(dict([(l,idx) for (idx,l) in enumerate(X[col].unique())]))\n",
    "    \n",
    "# Let us see how the features looks like now.\n",
    "print \"Features of Row 1 after numerical encoding\", X.iloc[1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the categorical data is now represented by numbers instead of characters. Now, the API will be able to process our data. \n",
    "\n",
    "However, it is not a good idea to have categorical data being represented by numeric values. This is because, the machine learning algorithm may perceive one class to be more important than the other, just because one class has a numerically larger class label. To overcome this problem, it is a common strategy to one-hot encode categorical data for machine learning problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot Encoding\n",
    "\n",
    "One-hot encoding is used for binarization of categorical data. In other words, one hot encoding will modify the data representation such that, if a feature has 'n' possible values, that feature will be represented by 'n' columns, each of which can contain either 1 or 0, indicating the presence or absence of the feature respectively.\n",
    "\n",
    "Let us consider an example. Look at the following data:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Colour</th>\n",
    "    <th>Shape</th> \n",
    "    <th>Size</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Red</td>\n",
    "    <td>Circle</td> \n",
    "    <td>Small</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>White</td>\n",
    "    <td>Box</td> \n",
    "    <td>Small</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Red</td>\n",
    "    <td>Box</td> \n",
    "    <td>Medium</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Black</td>\n",
    "    <td>Circle</td> \n",
    "    <td>Large</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "This data has 3 features: Colour, Shape and Size. If we were to encode each of the categorical features, this is one possible way of doing it :<br>\n",
    "<b>Colour</b> : Red = 0, White = 1, Black = 2 <br>\n",
    "<b>Shape</b> : Circle = 0, Box = 1 <br>\n",
    "<b>Size</b> : Small = 0, Medium = 1, Large = 2<br>\n",
    "\n",
    "The first row in the data(Colour = Red, Shape = Circle, Size = Small) can be numerically encoded as (0, 0, 0).  \n",
    "With one-hot encoding, the data will be represented as [[1, 0, 0],[1,0],[1,0,0]]. The inner list is representative of each feature, and 1 represents the presence of the feature value while 0 denotes the absence of it. Thus [1, 0, 0] means that Red is present while white and black are absent. \n",
    "\n",
    "On concatenation, the whole data can be represented as [1, 0, 0, 1, 0, 1, 0, 0]. This is the final one hot encoding of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at sklearn.preprocessing.OneHotEncoder, which we will be using to one-hot encode the categorical features in our mushroom dataset. To use this, the data being passed to the API needs to be in the form of numerical data. First an object of the OneHotEncoder class is created and then, to the fit_transform() function of the object, the data is passed, which will output the data with all the columns being encoded in one-hot encoding format.\n",
    "\n",
    "For more details, have a look at the documentation http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8124, 117)\n"
     ]
    }
   ],
   "source": [
    "# Enter code to create the one-hot encoding of the data.\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Saving the encoder to handle any data acquired later.\n",
    "encoder = OneHotEncoder().fit(X)\n",
    "\n",
    "# Transforming the data to one hot encoding\n",
    "X = encoder.transform(X)\n",
    "\n",
    "# Let us see the shape of X\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the number of columns has increased from 22 to 117.\n",
    "\n",
    "The data can now be passed to the sklearn Decision tree classifer API as it is one-hot encoded. Before doing so, it is essential to split the data into train and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into train and test\n",
    "\n",
    "It is essential to to split the data into train and test. This is done so because, if we validate the accuracy of the model on the training data itself, there is a chance that the model has learnt the noise in the training data and has associated it to be a part of the entire data. By validating the accuracy on the test dataset, we can quantify in a valid manner whether the model has learnt the generalisations of the data. \n",
    "\n",
    "This is done by using the train_test_split method present in sklearn.model_selection. The details for this follow below. \n",
    "\n",
    "For more details, have a look at the documentation http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here, X is the input features, y is the input labels. X_train and X_test are the train and test splits \n",
    "# respectively of the features. Likewise for the labels in y. The test_size gives the fraction of the total \n",
    "# data that is in the test data. The random_state is used to create the same split each time the function is called.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, your data is ready to be used to build a Decision Tree and evaluate it's efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your decision tree\n",
    "\n",
    "The next step is to fit a decision tree for your training data. For this we will use the DecisionTreeClassifier API. This can be imported from sklearn.tree . The API takes a number of parameters which will provide you with finer control on what the Decision Tree learns. For now, we will use the API with all of it's parameters set to their default values. The .fit() function is used to make the decision tree learn from the training data, which is passed as parameters to it. \n",
    "\n",
    "For more detials, have a look at the documentation http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create an object of the DecisionTreeClassifier class.\n",
    "model_tree = DecisionTreeClassifier()\n",
    "\n",
    "# To the created object, call the fit() function with the training data parameters. \n",
    "model_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and evaluate your model\n",
    "\n",
    "Now that your decision tree model has been trained. We will now compute the accuracy on the test dataset. This will give us a measure of how well that decision tree has been trained. A higher accuracy on the test dataset indicates that the Decision Tree has adequately learnt the right patterns in the data. The score() function used on the DecisionTreeClassifier object will output the accuracy. The features and labels of the test data is passed to the function as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "# The score() function is called with the test data as parameters. \n",
    "accuracy = model_tree.score(X_test, y_test)\n",
    "\n",
    "# The accuracy is output as a fraction. We multiply by 100 to obtain the accuracy in percentage.\n",
    "print accuracy*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get an accuracy of 100% on the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RandomForestClassifier to classify data \n",
    "\n",
    "Using the sklearn.ensemble.RandomForestClassifer is similar to using the DecisionTreeClassifier discussed above. The first stage would be to fit the training data on an object of RandomForestClassifer. This is done using the fit() function, passing the training data as parameters. Post that, the accuracy of the RandomForestClassifier can be obtained by using the score() function and passing the test data as parameters. This is done in the code given below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create an object of RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Fit your training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Check the accuracy \n",
    "y_acc = clf.score(X_test, y_test)\n",
    "print y_acc*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
