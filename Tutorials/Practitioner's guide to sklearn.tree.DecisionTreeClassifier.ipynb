{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maintaining high accuracy on Decision Tree Classifier\n",
    "\n",
    "The goal of a good machine learning model is to generalize well from the training data to any data from the problem domain. This allows us to make predictions on data the model has never seen. It is possible that over the course of applying machine learning algoritms to solve problems, the manner in which the model learns from the training dataset can result in some abnormalities. The 2 commonly encountered scenarios when dealing with real world data are Overfitting and Underfitting. Overfitting and underfitting are the two biggest causes for poor performance of machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting\n",
    "\n",
    "Overfitting refers to a model that models the training data too well.\n",
    "\n",
    "Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model. The problem is that these concepts do not apply to new data and negatively impact the model's ability to generalize the common features of the data. \n",
    "\n",
    "In practice, overfitting results in high accuracies on the training data and low accuracies on the test data.\n",
    "\n",
    "The Decision Boundary for one such model that has overfit to it's training data is given below. Notice how the model has managed to account for every data point in the training data while not generalizing well enough.\n",
    "![Overfitting Dataset][logo]\n",
    "\n",
    "[logo]: overfitting.png \"Logo Title Text 2\"\n",
    "\n",
    "Decision Trees are quite easily subjected to overfitting. The methods to deal with Overfitting are described below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overcoming Overfitting in Decision Trees.\n",
    "\n",
    "Dealing with overfitting involves using techniques that increase the generalisation of the learning by the model. In Decision trees, using a good stopping criteria can play a vital role in preventing overfitting. The stopping criteria can be specified by heursitics such as, the depth of the tree, the number of leaf nodes in the tree, the minimum number of data points needed for a split to happen, among others. In this tutorial, we will cover optimising for overfitting in the context of using the parameters made available by sklearn.tree.DecisionTreeClassifier()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### API Parameters\n",
    "\n",
    "sklearn.tree.DecisionTreeClassifier has a number of parameters, that can be tuned whenever necessary to achieve desirable results. An explanation for some important parameters follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### max_depth \n",
    "This parameter decides the maximum levels that the decision tree can grow to. If it is left to the default value of None, then the decision tree will grow until all the leaf nodes are pure or until any further splitting is not possible. By restricting the depth of the tree, one can prevent the model from learning training data noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### min_samples_split \n",
    "This parameter sets the minimum number of samples that are needed to split an internal node. By default, this value is set to 2, which means that, only if there are 2 or more data points at an internal node, a split will happen, creating 2 leaf nodes. This value can be increased to prevent instances of overfitting, since it will require more data for a split to happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### min_samples_leaf  \n",
    "This parameter sets the minimum number of data points that need to be present in a leaf node. The default value for this parameter is 1, which means that, it is sufficient even if 1 data point is present at leaf nodes. By increasing the value of this parameter, the number of leaf nodes and hence, the size of the tree can restricted, thus, reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### max_leaf_nodes \n",
    "This parameter is used to set the maximum number of leaf nodes in the decision tree. The default option is to have no barrier on the maximum number of leaf nodes that can exist in the Decision Tree. By controlling the number of leaf nodes in the tree, the growth of the tree can be controlled, and thus, overfitting of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### min_impurity_decrease \n",
    "By setting the minimum decrease in impurity post splitting of an internal node, one can control the rate at which the tree grows. and thus, serves as a manner of controlling the overfitting to the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of those parameters serve as a means to control the complexity of the tree structure. In essence, the less complex the model, the lesser are the chances of having ovefitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Underfitting\n",
    "\n",
    "Underfitting refers to a model that can neither model the training data nor generalize to new data.\n",
    "\n",
    "An underfit machine learning model is not a suitable model and will be obvious as it will have poor performance on the training data. Underfitting can occur if the model used is too simple or if the features in the data are not structured in a way that would allow differentiating between different class labels. \n",
    "\n",
    "The decision boundary for a model suffering fron underfitting is given below:\n",
    "\n",
    "\n",
    "<img src=\"Underfitting.png\" alt=\"Drawing\" style=\"width: 393px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overcoming Underfitting in Decision Trees.\n",
    "\n",
    "Dealing with underfitting involves using techniques that increase the complexity of the learning by the model. \n",
    "\n",
    "In Decision trees, using aggressive values for some parameters such as maximum depth of the tree, may also result in underfitting. For example, if the max depth of the tree is set to 2, then, the tree is not complex enough to make sense of the data and hence, suffers from underfitting.  \n",
    "\n",
    "Underfitting may also result if the features for the data are not clearly representative of the different class labels. In other words, the data for each of the class labels is not distinguishable enough for the model being used. Better feature engnieering can avoid this situation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
